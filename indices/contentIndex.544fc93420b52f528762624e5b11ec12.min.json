{"/":{"title":"Balbonario","content":"\nWelcome to the *Balbonario*, a collection of known facts in hard sciences and related fields that you can use as a fast reference or to explore those fields.\n\nSome indices pages that you may want to start from are:\n- [Classical Machine Learning](indices/classical-machine-learning.md) about the methods and models of the basic parts of statistics and machine learning.\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null},"/indices/classical-machine-learning":{"title":"Classical Machine Learning","content":"Classical Machine Learning refers to the more ancient part of machine learning, which has many relations with statistics, and mainly deals with the problem of [Function Approximation](machine-learning/function-approximation.md) via [Convex Optimization](_drafts/convex-optimization.md) of some basic models.\n\n## Classical Models\n- [K-Nearest Neighbours](machine-learning/k-nearest-neighbours.md)\n- [Linear Regression](machine-learning/linear-regression.md)\n\n## Books\n- [Hastie, Tibshirani, Friedman - The Elements of Statistical Learning](references/hastie-tibshirani-friedman-the-elements-of-statistical-learning.md)\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null},"/machine-learning/function-approximation":{"title":"Function Approximation","content":"The setting of Function Approximation is the following: suppose you are given two spaces $X$ and $Y$, which model respectively the \"independent variable\" and the \"dependent variable\" or, more clearly, $X$ is the variable you can observe (such as concentrations of various molecules in blood) and $Y$ is the variable you are interested in predicting (such as the possibility of a stroke to that person).\n\nWe assume that the $n$ data samples $\\left\\{z_i = (x_i, y_i)\\right\\}_{i=1}^n$ that we have in our concrete problem come from a joint (regular enough) probability distribution $\\mathcal{D}$ on $X \\times Y$ and we write it as $(x_i, y_i) \\sim \\mathcal{D} \\in \\mathcal{P}(X \\times Y)$.\nThe problem is to find a function $f$ in a certain class $\\mathcal{F}$[^function-class] that well-approximates the empirical law between $x_i$ and $y_i$, i.e. we would like that\n$$f(x) := \\mathbb{E}_{(X, Y) \\sim \\mathcal{D}}\\left[Y \\mid X = x\\right].$$\n\n[^function-class]: In this case the function class $\\mathcal{F}$ is used to enforce some regularity conditions or architectural biases on the kind of function that $f$ is allowed to be, e.g. one can force $f$ to be continuous or to be linear or to be written as a convolution in a certain measure space.\n\nIn order to find such a function we need to specify a [Loss Function](machine-learning/loss-function.md) $\\mathcal{L}: \\mathcal{F} \\rightarrow \\mathbb{R}$. The loss function is typically assumed to be convex so that standard [Convex Optimization](_drafts/convex-optimization.md) procedures apply, to guarantee good convergence results.\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null},"/machine-learning/k-nearest-neighbours":{"title":"K-Nearest Neighbours","content":"$k$-nearest neighbours is a [Function Approximation](machine-learning/function-approximation.md) algorithm which assumes that the function to approximate is locally constant, and thus approximates the output variable $\\hat y$ with a mean of the $k$ nearest neighbours, i.e.\n$$\\hat y(x) := \\frac1k \\sum_{i \\in N_k(x)} y_i$$\nwhere $N_k(x)$ represents the set of the $k$ neighbours of $x$, and which necessitates of a metric over $X$ to be specified.\n\nThe number of effective parameters of the method is $n/k$, where $n$ is the number of datapoints, since the method essentially fits a mean to each grouping of $k$ elements, thus fitting $n/k$ means.\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null},"/machine-learning/linear-regression":{"title":"Linear Regression","content":"Linear regression is a [Function Approximation](machine-learning/function-approximation.md) algorithm which assumes that your function is linear, i.e. $\\hat Y = \\beta X$ where $X \\in \\mathbb{R}^{p \\times n}, \\hat Y \\in \\mathbb{R}^{m \\times n}, \\beta \\in \\mathbb{R}^{m \\times p}$, and $\\beta$ is the matrix of coefficients to be fit.\nThe method has $mp$ effective parameters.\n\nIn order to fit the data you have to specify a [Loss Function](machine-learning/loss-function.md).\nThe common one in this setting is [Mean Squared Error](machine-learning/mean-squared-error.md) on the data, i.e. we are interested in solving the minimization problem\n$$\\min_\\beta \\frac1n \\sum_{i \\le n} (y_i - \\beta x_i)^2$$\nfrom which we can derive the normal equations $X^T (y - X \\beta) = 0$ and thus the solution $\\beta = (X^T X)^{-1} X^T y$.\n\nOne can do the same calculations also in the distributional setting and obtain $\\beta = \\mathbb{E}\\left[XX^T\\right]^{-1} \\mathbb{E}\\left[XY\\right]$.\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null},"/machine-learning/loss-function":{"title":"Loss Function","content":"#todo How to choose a loss function?\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null},"/machine-learning/mean-squared-error":{"title":"Mean Squared Error","content":"The mean squared error is a [Loss Function](machine-learning/loss-function.md) typically used in classical [Function Approximation](machine-learning/function-approximation.md).\nGiven $n$ datapoints $(x_i, y_i)$ and relative predictions made by a learning algorithm $\\hat y_i = f(x_i)$, the loss function is defined as:\n$$\\mathcal{L}(f) := \\frac1n \\sum_{i = 1}^n \\left(y_i - f(x_i)\\right)^2.$$\n\nIt has the benefit of being differentiable giving linear equations to be solved, which makes for easy analytical solutions of the methods that use it.\nMoreover, it is the form of [Maximum Likelihood Estimation](None) when one assumes that the real data-generating model is contained in the class $\\mathcal{F}$ one is optimizing over, and that measurement errors in the $y_i$ are distributed according to a normal gaussian, i.e. under the assumption that $\\exists f \\in \\mathcal{F} \\quad y_i = f(x_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0, 1)$.\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null},"/references/hastie-tibshirani-friedman-the-elements-of-statistical-learning":{"title":"Hastie, Tibshirani, Friedman - The Elements of Statistical Learning","content":"#todo This is a well-known introductory book in the field of [Classical Machine Learning](indices/classical-machine-learning.md).\n","lastmodified":"2022-08-31T08:24:18.177031287Z","tags":null}}