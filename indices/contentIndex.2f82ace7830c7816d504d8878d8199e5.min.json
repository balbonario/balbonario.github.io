{"/":{"title":"Balbonario","content":"\nWelcome to the *Balbonario*, a collection of known facts in hard sciences and related fields that you can use as a fast reference or to explore those fields.\n\nSome indices pages that you may want to start from are:\n- [Classical Machine Learning](indices/classical-machine-learning.md) about the methods and models of the basic parts of statistics and machine learning.\n- [Algorithms and Data Structures](indices/algorithms-and-data-structures.md) about ingenious tricks to solve various computational problems in optimal space-time complexity.\n","lastmodified":"2022-09-02T09:37:49.836775878Z","tags":null},"/algorithms-and-data-structures/segment-trees":{"title":"Segment Trees","content":"#todo \n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/algorithms-and-data-structures/square-root-decomposition":{"title":"Square Root Decomposition","content":"#todo Refers to the square root decomposition in data structures as a way of using $O(\\sqrt{n})$ memory to answer queries in $O(\\sqrt{n})$ time.\nWe can compare it to building full-trees like [Segment Trees](algorithms-and-data-structures/segment-trees.md) that instead have $O(n)$ memory to answer queries in $O(\\log n)$ time.\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/indices/algorithms-and-data-structures":{"title":"Algorithms and Data Structures","content":"#todo \n\n## Intermediate Tricks\n- [Square Root Decomposition](algorithms-and-data-structures/square-root-decomposition.md)\n","lastmodified":"2022-09-02T09:37:49.848776112Z","tags":null},"/indices/classical-machine-learning":{"title":"Classical Machine Learning","content":"Classical Machine Learning refers to the more ancient part of machine learning, which has many relations with statistics, and mainly deals with the problem of [Function Approximation](machine-learning/function-approximation.md) via [Convex Optimization](optimization-theory/convex-optimization.md) of some basic models.\n\n## Theory\n- [Bias-Variance Decomposition](machine-learning/bias-variance-decomposition.md)\n- [Function Approximation](machine-learning/function-approximation.md)\n- [Regularization](machine-learning/regularization.md)\n- [The Curse of Dimensionality](machine-learning/the-curse-of-dimensionality.md)\n\n## Classical Models\n- [K-Nearest Neighbours](machine-learning/k-nearest-neighbours.md)\n- [Linear Regression](machine-learning/linear-regression.md)\n\n## Practice\n- Model ensembling methods: [Bagging](machine-learning/bagging.md), [Boosting](machine-learning/boosting.md), [Stacking](machine-learning/stacking.md).\n- Model evaluation metrics: [Confusion Matrix](machine-learning/confusion-matrix.md), [Cross-Validation](machine-learning/cross-validation.md), [ROC Curve](machine-learning/roc-curve.md).\n\n## Books\n- [Hastie, Tibshirani, Friedman - The Elements of Statistical Learning](references/hastie-tibshirani-friedman-the-elements-of-statistical-learning.md)\n","lastmodified":"2022-09-02T09:37:49.848776112Z","tags":null},"/linear-algebra/fast-updates-to-matrix-operations":{"title":"Fast Updates to Matrix Operations","content":"Given a matrix operation $h$ and a fixed matrix $A$ there are in certain cases better ways to compute $h(A + uv^T)$ from $h(A)$ than to recompute it. This formulas allow for faster online algorithms and are collected in this page.\n\n## Sherman-Morrison Formula\nThe formula allows to compute the inverse of the sum of an invertible matrix $A$ and the outer product $uv^T$ of two vectors $u, v$.\n\n\u003e [!LEMMA] Sherman-Morrison\n\u003e Suppose $A \\in \\mathbb{R}^{n \\times n}$ is an invertible square matrix and $u, v \\in \\mathbb{R}^n$ are column vectors.\n\u003e Then $A + uv^T$ is invertible if and only if $1 + v^T A^{-1} u \\neq 0$, and in this case it holds:\n\u003e $$(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1} uv^T A^{-1}}{1 + v^T A^{-1} u}.$$\n\nA generalization of this identity for matricial updates is the following.\n\u003e [!LEMMA] Woodbury Identity\n\u003e Given a square invertible $n \\times n$ matrix $A$, two $n \\times k$ matrices $U, V$ and an invertible $k \\times k$ matrix $W$ we have the identity\n\u003e $$(A + U W V^T)^{-1} = A^{-1} - A^{-1} U \\left(W^{-1} + V^T A^{-1} U\\right)^{-1} V^T A^{-1}.$$\n\n## Matrix Determinant Lemma\nThe formula allows to compute the determinant of a sum of an invertible matrix $A$ and the outer product of two vectors $uv^T$.\n\n\u003e [!LEMMA] Matrix Determinant Lemma\n\u003e Let $A$ be an invertible square matrix and $u, v$ be column vectors. Then it holds:\n\u003e $$\\text{det}(A + uv^T) = (1 + v^T A^{-1} u)\\ \\text{det}(A).$$\n\u003e \n\u003e In the case where we have $U, V$ as two $n \\times m$ matrices and $W$ an additional $m \\times m$ invertible matrix we have the identity:\n\u003e $$\\text{det}(A + UWV^T) = \\text{det}(W^{-1} + V^T A^{-1} U)\\ \\text{det}(W)\\ \\text{det}(A).$$\n\n#### Others\n- [A Stable and Efficient Algorithm for the Rank-one Modification of the Symmetric Eigenproblem](http://www.cs.yale.edu/publications/techreports/tr916.pdf) [(Archived)](https://web.archive.org/web/20220901/http://www.cs.yale.edu/publications/techreports/tr916.pdf)\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/machine-learning/bagging":{"title":"Bagging","content":"Bagging (**B**ootstrap **Agg**regat**ing**) is a set of model ensembling methods where multiple models of the same learner type are built in parallel over different random variations and their predictions aggregated together by averaging or majority vote.\n\nBagging procedures are based on [Bootstrapping](machine-learning/bootstrapping.md) theory, and exploit variations in randomness in the data samples and randomness in the model construction.\nIn the case of random forests for example, randomness is in the sequential choices of the features to split the dataset.\n\n![bagging-schema.svg](None)\n\nAveraging the predictions of multiple weak learners doesn't change the average answer but reduces the variances of the learners if the data subsamples can be considered independent (i.e. if their size with respect to the whole dataset is small as not to have too much overlap).\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/bias-variance-decomposition":{"title":"Bias-Variance Decomposition","content":"The Bias-Variance Decomposition is a mathematical decomposition of the distributional error.\nFor the [Mean Squared Error](machine-learning/mean-squared-error.md) loss we have that\n$$\n\\begin{align*}\n\\text{MSE}(x_0) \u0026 = \\mathbb{E}_\\mathcal{D}\\left[f(x_0) - \\hat y_0\\right]^2 \\\\\n\u0026 = \\mathbb{E}_\\mathcal{D}\\left[\\hat y_0 - \\mathbb{E}_\\mathcal{D}[\\hat y_0]\\right]^2 + \\left(\\mathbb{E}_\\mathcal{D}[\\hat y_0]^2 - f(x_0)\\right)^2 \\\\\n\u0026 = \\text{Var}_\\mathcal{D}(\\hat y_0) + \\text{Bias}(\\hat y_0)^2\n\\end{align*}\n$$\nwhere the bias is the displacement of the mean prediction $\\hat y_0$ from the true target $y_0 = f(x_0)$, and the variance is the variance of the prediction.\n\nIn the case where we assume that there is noise in our measurement of data we get a third additive term which is called irreducible error or noise.\n\nTo get the lowest errors we would like to reduce both the bias and the variance, but it happens that we can't do this so easily, and sometimes not at all.\n\nFor a fixed model of the data (e.g. [Linear Regression](machine-learning/linear-regression.md)) we can analytically compute the minimum MSE for different settings of the number of parameters of the fit (i.e. the number of coefficients for linear regression).\nIf one does this, it can be seen that there is a [Bias-Variance Tradeoff](machine-learning/bias-variance-tradeoff.md), i.e. one has to acquire a bit of variance if we want to decrease the bias and viceversa.\n\n#### Deepenings\n- [A Unified Bias-Variance Decomposition](https://homes.cs.washington.edu/~pedrod/bvd.pdf) [(Archived)](https://web.archive.org/web/20220902/https://homes.cs.washington.edu/~pedrod/bvd.pdf)\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/bias-variance-tradeoff":{"title":"Bias-Variance Tradeoff","content":"The *bias-variance tradeoff* refers to a common phenomenon occurring in many statistical models where it is not possible to decrease both the bias and the variance of a model[^bias-variance-definition] by changing the number of parameters of a model.\n\n[^bias-variance-definition]: For definitions of the bias and variance of a model see [Bias-Variance Decomposition](machine-learning/bias-variance-decomposition.md).\n\nThis phenomenon is true in the classical regime where the number of free parameters is less than the number of datapoints, but stops to hold for overparameterized models where the situation reverts with both bias and variance decreasing in what has been called [Double Descent](machine-learning/double-descent.md).\n\nThe crux of that phenomenon is that if there is some kind of strong regularization (which may be implicit in the learning algorithm) then the algorithm will converge to models which perfectly interpolate the training data, while also doing good on test data.\nFrom a mild overparameterization onward, each new parameter is used in a more complicated kind of smoothing of the test data, which may actually fit the test data better.\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/machine-learning/boosting":{"title":"Boosting","content":"Boosting is a sequential ensembling method to aggregate different weak models. The idea is to fit models iteratively such that the training of successive models is focused on the most difficult observations to fit up to now, so that the bias of the composite learner is reduced, and thus it is a technique usually used on models with low variance but high bias, such as shallow decision trees.\n\nThe various boosting techniques differ on the creation and aggregation of the weak learners during the sequential process, typically updating the weights of the weak learners predictors and the distributional resampling for successive models to put more emphasis on misclassified samples.\n\n![boosting-schema.svg](None)\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/bootstrapping":{"title":"Bootstrapping","content":"Bootstrapping is a technique to estimate the variability of a statistics from the empirical variability of that statistic between subsamples (extracted with replacement) of the empirical data, rather than from parametric assumptions.\n\nTwo assumptions have to hold true for bootstrapping to be useful:\n- The size of the initial dataset should be large enough to capture most of the complexity of the underlying distribution so that sampling from the dataset is a good approximation of sampling from the real distribution.\n- The size of the subsamples should be small compared to the size of the dataset so that samples are not too much correlated.\n\n#todo \n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/machine-learning/categorical-cross-entropy":{"title":"Categorical Cross-Entropy","content":"#todo \n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/confusion-matrix":{"title":"Confusion Matrix","content":"A confusion matrix is a good but simple metric that should always be used when dealing with classification problems.\nThe matrix has on one axis the true label and on the other axis the predicted label. In each cell it is written the number of points with that true label and that predicted label.\n\nFrom the confusion matrix multiple relevant quantities such as accuracy, precision and recall can be calculated.\nSince simple accuracy can be misleading when one is analyzing a dataset with imbalanced classes, it is important to look at the whole matrix.\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/cross-validation":{"title":"Cross-Validation","content":"Cross-Validation refers to the practice of performing multiples independent splits of the dataset into a training set and a test set, and repeat the training of a new model and its evaluation on each independent split, to later average the results.\n\nIn a way this allows to use the $n$ initial datapoints multiple times and amounts to an estimation of the mean performance of the model under different initial samples.\nThis method can thus be used to compare the performance of different predictive modelling procedures.\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/machine-learning/double-descent":{"title":"Double Descent","content":"Double Descent is a phenomenon that occurs for overparameterized models: classical statistics advises that there is a [Bias-Variance Tradeoff](machine-learning/bias-variance-tradeoff.md) when the complexity of the model changes, and that one can lose a bit of bias by trading it with an increase in variance and viceversa.\n\nWhat is instead observed is that when one moves to overparameterized models (i.e. where the number of effective degrees of freedom is higher than the number of datapoints to fit) things change and in the interpolating regime both bias and variance decrease, to an extent that makes such model usually work better than classical ones.\n\n![double-descent-curve.png](None)\n\nIntuitively, the phenomenon happens because at the interpolation threshold, there is often a *single* choice of a model that fits the data, and thus even small amount of noise in the data samples will make the model vary by quite a bit, providing a real overfit.\n\nWhen the models are instead overparameterized by a bit, we have a variety of models that completely fit the data, and regularization (which is usually implicit in the learning algorithm) is essential to find one that generalize. When adding some noise in the data, the model doesn't change much, and thus doesn't overfit to the idiosyncrasies in the data.\n\nWhat validates this view is the fact that by adding the appropriate [Regularization](machine-learning/regularization.md), the test error peak at the interpolation threshold vanishes and one gets almost monotonic performance.\n\n#### Deepenings\n- [Double Descent: A Mathematical Explanation](https://mlu-explain.github.io/double-descent2/) [(Archived)](https://web.archive.org/web/20220831/https://mlu-explain.github.io/double-descent2/)\n- [Optimal Regularization can Mitigate Double Descent](https://arxiv.org/pdf/2003.01897.pdf) [(Archived)](https://web.archive.org/web/20220901/https://arxiv.org/pdf/2003.01897.pdf)\n- [Kernel Regression in High Dimensions: Refined Analysis beyond Double Descent](http://proceedings.mlr.press/v130/liu21b/liu21b.pdf) [(Archived)](https://web.archive.org/web/20220901/http://proceedings.mlr.press/v130/liu21b/liu21b.pdf)\n- [A Modern Take on the Bias-Variance Tradeoff in Neural Networks](https://arxiv.org/pdf/1810.08591.pdf) [(Archived)](https://web.archive.org/web/20220901/https://arxiv.org/pdf/1810.08591.pdf)\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/machine-learning/empirical-risk-minimization":{"title":"Empirical Risk Minimization","content":"#todo \n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/function-approximation":{"title":"Function Approximation","content":"The setting of Function Approximation is the following: suppose you are given two spaces $X$ and $Y$, which model respectively the \"independent variable\" and the \"dependent variable\" or, more clearly, $X$ is the variable you can observe (such as concentrations of various molecules in blood) and $Y$ is the variable you are interested in predicting (such as the possibility of a stroke to that person).\n\nWe assume that the $n$ data samples $\\left\\{z_i = (x_i, y_i)\\right\\}_{i=1}^n$ that we have in our concrete problem come from a joint (regular enough) probability distribution $\\mathcal{D}$ on $X \\times Y$ and we write it as $(x_i, y_i) \\sim \\mathcal{D} \\in \\mathcal{P}(X \\times Y)$.\nThe problem is to find a function $f$ in a certain class $\\mathcal{F}$[^function-class] that well-approximates the empirical law between $x_i$ and $y_i$ in a way that allows us to suffer the least losses.\n\nIn fact, we typically we have an idea of the cost associated with a given misprediction, and we can model this as a [Loss Function](machine-learning/loss-function.md) $\\ell: X \\times Y \\times Y \\rightarrow \\mathbb{R}$.\nThen we are interested in the minimization of the cost over the whole input distribution, i.e. we are interested in solving the problem\n$$\\min_{f \\in \\mathcal{F}}\\ \\mathbb{E}_{(X, Y) \\sim \\mathcal{D}}\\left[\\ell(X, Y, f(X))\\right].$$\n\n[^function-class]: In this case the function class $\\mathcal{F}$ is used to enforce some regularity conditions or architectural biases on the kind of function that $f$ is allowed to be, e.g. one can force $f$ to be continuous or to be linear or to be written as a convolution in a certain measure space.\n\nWe typically need to find a proxy for such a quantity with the data samples that we are given. Usually this proxy quantity is given by [Empirical Risk Minimization](machine-learning/empirical-risk-minimization.md) with some kind of [Regularization](machine-learning/regularization.md), and we can thus write the function to be minimized as\n$$\\mathcal{L}(f) := \\frac1n \\sum_{i=1}^n \\ell(x_i, y_i, f(x_i)) + \\lambda \\Omega(f)$$\nwhere $\\ell$ and $\\Omega: \\mathcal{F} \\rightarrow \\mathbb{R}$ (the *regularization term*) are assumed to be convex functions so that standard [Convex Optimization](optimization-theory/convex-optimization.md) procedures apply, and it is feasible to find a solution to the minimization problem.\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/k-nearest-neighbours":{"title":"K-Nearest Neighbours","content":"$k$-nearest neighbours is a [Function Approximation](machine-learning/function-approximation.md) algorithm which assumes that the function to approximate is locally constant, and thus approximates the output variable $\\hat y$ with a mean of the $k$ nearest neighbours, i.e.\n$$\\hat y(x) := \\frac1k \\sum_{i \\in N_k(x)} y_i$$\nwhere $N_k(x)$ represents the set of the $k$ neighbours of $x$, and which necessitates of a metric over $X$ to be specified.\n\nThe number of effective parameters of the method is $n/k$, where $n$ is the number of datapoints, since the method essentially fits a mean to each grouping of $k$ elements, thus fitting $n/k$ means.\n\nThis method presents us with a notable problem when going into high dimensions, since if we need $N$ points to have good classification on a single-dimensional $X$, we need $N^d$ points to have good classification on a $d$-dimensional $X$.\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/machine-learning/kernel-methods":{"title":"Kernel Methods","content":"#todo \n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/linear-regression":{"title":"Linear Regression","content":"Linear regression is a [Function Approximation](machine-learning/function-approximation.md) algorithm which assumes that your function is linear, i.e. $\\hat Y = \\beta X$ (and can also have a bias term) where $X \\in \\mathbb{R}^{p \\times n}, \\hat Y \\in \\mathbb{R}^{m \\times n}, \\beta \\in \\mathbb{R}^{m \\times p}$, and $\\beta$ is the matrix of coefficients to be fit.\nThe method has $mp$ effective parameters.\n\nIn order to fit the data you have to specify a [Loss Function](machine-learning/loss-function.md).\nThe common one in this setting is [Mean Squared Error](machine-learning/mean-squared-error.md) on the data, i.e. we are interested in solving the minimization problem\n$$\\min_\\beta \\frac1n \\sum_{i \\le n} (y_i - \\beta x_i)^2$$\nfrom which we can derive the normal equations $X^T (y - X \\beta) = 0$ and thus the solution $\\hat\\beta = (X^T X)^{-1} X^T y$, and the predicted values at the training inputs are $\\hat y = X \\hat \\beta$.\n\nOne can do the same calculations also in the distributional setting and obtain $\\beta = \\mathbb{E}\\left[XX^T\\right]^{-1} \\mathbb{E}\\left[XY\\right]$.\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/loss-function":{"title":"Loss Function","content":"#todo How to choose a loss function?\n\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/mean-squared-error":{"title":"Mean Squared Error","content":"The mean squared error is a [Loss Function](machine-learning/loss-function.md) typically used in classical [Function Approximation](machine-learning/function-approximation.md).\nGiven $n$ datapoints $(x_i, y_i)$ and relative predictions made by a learning algorithm $\\hat y_i = f(x_i)$, the loss function is defined as:\n$$\\mathcal{L}(f) := \\frac1n \\sum_{i = 1}^n \\left(y_i - f(x_i)\\right)^2.$$\n\nIt has the benefit of being differentiable giving linear equations to be solved, which makes for easy analytical solutions of the methods that use it.\nMoreover, it is the form of [Maximum Likelihood Estimation](statistical-learning-theory/maximum-likelihood-estimation.md) when one assumes that the real data-generating model is contained in the class $\\mathcal{F}$ one is optimizing over, and that measurement errors in the $y_i$ are distributed according to a normal gaussian, i.e. under the assumption that $\\exists f \\in \\mathcal{F} \\quad y_i = f(x_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0, 1)$.\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/overfitting-and-underfitting":{"title":"Overfitting and Underfitting","content":"Overfitting occurs when the model is so specific to the data on which it has been trained that it is no longer applicable to other random samplings from the dataset, and thus has an almost zero train error but with a high test error.\n\nUnderfitting on the other hand happens when the model is too simple to accurately capture the relationships between its features $X$ and the label $Y$, and is manifested by having high train error and high test error.\n\nThese are generic terms to point out possible problems with models but there is no \"measure of overfitting\".\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/regularization":{"title":"Regularization","content":"#todo Describe regularization\n\n#### Deepenings\n- [On Representer Theorems and Convex Regularization](https://ydecastro.github.io/research/paper23.pdf) [(Archived)](https://web.archive.org/web/20220831/https://ydecastro.github.io/research/paper23.pdf)\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/roc-curve":{"title":"ROC Curve","content":"The ROC (Receiver Operating Characteristic) Curve is defined with respect to a given class and encodes the curve of false positive, true positive when varying a threshold $T$ for a one-versus-rest classifier.\n\nIn simpler words, suppose that for a given point $x$ we have a model that outputs the probability that this point belongs to a given class $C$ and denote it by $P(C \\mid x)$.\nBased on this probability we can define a decision rule by setting a threshold $T$ and saying that $x$ belongs to class $C$ if and only if $P(C \\mid x) \\ge T$.\n\nEach value of the threshold generates a point given by $(\\text{false positive ratio}, \\text{true positive ratio})$ which defines a curve.\nA good model will have a curve that increases quickly from zero to one.\n\nA metric that compresses the information in the ROC curve into a single scalar is the AUROC or Area Under the ROC curve, which tends towards $1.0$ for the best case and towards $0.5$ for the worst cases.\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/machine-learning/stacking":{"title":"Stacking","content":"Stacking is a model ensembling technique that is focused on aggregating heterogeneous weak learners with a meta-model that aggregates the decisions of weak models.\n\nFor a practical example, suppose that we have a [K-Nearest Neighbours](machine-learning/k-nearest-neighbours.md) classifier and a [Logistic Regression](None) to aggregate, and we decide to learn a [Neural Network](None) as meta-model.\nThen we will split the learning data in two folds; use the first fold to fit the base learners, and make predictions with those on the second fold of data; finally fit the meta-model on the second fold of data, using the predictions by the weak learners as inputs.\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/machine-learning/the-curse-of-dimensionality":{"title":"The Curse of Dimensionality","content":"The *curse of dimensionality* is a phenomenon that makes it more difficult to use machine learning problems in high dimensions due to the need of much more data.\n\nApproximately, if $n$ samples provide good results in a single dimension, then $n^d$ samples are needed to provide good results when the input has $d$ dimensions, *under the assumption that the samples are independently distributed on each dimension*.\n\nAnother problem in high dimensions is that euclidean distances loose meaning, since distances between two random points have a peaked distribution due to [Concentration Inequalities](statistical-learning-theory/concentration-inequalities.md) of random variables (in this case the random components of the data points).\nThese render ineffective any approach based on [K-Nearest Neighbours](machine-learning/k-nearest-neighbours.md) or similarity of the input data.\n\nThe curse of dimensionality does not apply to models with strong architectural biases, because they don't have the capacity to fit all possible idiosyncrasies of the data, and thus actually perform better in high dimensions.\n","lastmodified":"2022-09-02T09:37:49.840775956Z","tags":null},"/optimization-theory/convex-optimization":{"title":"Convex Optimization","content":"#todo\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/references/hastie-tibshirani-friedman-the-elements-of-statistical-learning":{"title":"Hastie, Tibshirani, Friedman - The Elements of Statistical Learning","content":"A well-known introductory book in the field of [Classical Machine Learning](indices/classical-machine-learning.md), which talks about basic topics such as [Bias-Variance Decomposition](machine-learning/bias-variance-decomposition.md), [The Curse of Dimensionality](machine-learning/the-curse-of-dimensionality.md), and standard models such as [Linear Regression](machine-learning/linear-regression.md), [K-Nearest Neighbours](machine-learning/k-nearest-neighbours.md).\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/statistical-learning-theory/concentration-inequalities":{"title":"Concentration Inequalities","content":"#todo \n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null},"/statistical-learning-theory/maximum-likelihood-estimation":{"title":"Maximum Likelihood Estimation","content":"A general method to derive a loss function when an analytical form for the data-generating probability is known.\n\nSuppose that we have a random sample $y_i$ for $i = 1, \\ldots, n$ from a density $\\text{Pr}_\\theta(y)$ which is indexed by some parameters $\\theta$. The log-probability of the observed sample is:\n$$\\mathcal{L}(\\theta) := \\sum_{i=1}^n \\log \\text{Pr}_\\theta(y_i).$$\n\nThe principle of maximum likelihood assumes that the most reasonable values for $\\theta$ are those for which the probability of the observed sample is largest.\n\n## Derived Losses\nThis principle is at the base of the definition of both [Mean Squared Error](machine-learning/mean-squared-error.md) and [Categorical Cross-Entropy](machine-learning/categorical-cross-entropy.md).\n\nIndeed, consider the additive error model $Y = f_\\theta(X) + \\varepsilon$ where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. This model is equivalent to maximum likelihood using the conditional likelihood\n$$\\text{Pr}(Y \\mid X, \\theta) = \\mathcal{N}(f_\\theta(X), \\sigma^2)$$\nwhose log-likelihood gives rise to [Mean Squared Error](machine-learning/mean-squared-error.md).\n\nConsider now what happens for a multinomial likelihood for the regression function $\\text{Pr}(G \\mid X)$ where $G$ is a categorical output; suppose we have a model $\\text{Pr}(G = g_k \\mid X = x) = p_{k, \\theta}(x)$ for $k = 1, \\ldots, K$ for the conditional probability of each class given $X$.\nThen the log-likelihood is\n$$\\mathcal{L}(\\theta) = \\sum_{i=1}^n \\log p_{g_i, \\theta}(x_i)$$\nwhich gives rise to [Categorical Cross-Entropy](machine-learning/categorical-cross-entropy.md) loss.\n","lastmodified":"2022-09-02T09:37:49.844776034Z","tags":null}}