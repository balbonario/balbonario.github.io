<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistical-learning-theories on</title><link>https://balbonario.github.io/statistical-learning-theory/</link><description>Recent content in Statistical-learning-theories on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 02 Sep 2022 11:37:09 +0200</lastBuildDate><atom:link href="https://balbonario.github.io/statistical-learning-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>Concentration Inequalities</title><link>https://balbonario.github.io/statistical-learning-theory/concentration-inequalities/</link><pubDate>Fri, 02 Sep 2022 11:37:09 +0200</pubDate><guid>https://balbonario.github.io/statistical-learning-theory/concentration-inequalities/</guid><description>#todo</description></item><item><title>Maximum Likelihood Estimation</title><link>https://balbonario.github.io/statistical-learning-theory/maximum-likelihood-estimation/</link><pubDate>Fri, 02 Sep 2022 11:37:09 +0200</pubDate><guid>https://balbonario.github.io/statistical-learning-theory/maximum-likelihood-estimation/</guid><description>A general method to derive a loss function when an analytical form for the data-generating probability is known.
Suppose that we have a random sample $y_i$ for $i = 1, \ldots, n$ from a density $\text{Pr}\theta(y)$ which is indexed by some parameters $\theta$.</description></item></channel></rss>