<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Balbonario on</title><link>https://balbonario.github.io/</link><description>Recent content in Balbonario on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://balbonario.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Bias-Variance Decomposition</title><link>https://balbonario.github.io/machine-learning/bias-variance-decomposition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/bias-variance-decomposition/</guid><description>The Bias-Variance Decomposition is a kind of mathematical decomposition of the distributional error. For the Mean Squared Error loss we have that $$ \begin{align*} \text{MSE}(x_0) &amp;amp; = \mathbb{E}\mathcal{D}\left[f(x_0) - \hat y_0\right]^2 \ &amp;amp; = \mathbb{E}\mathcal{D}\left[\hat y_0 - \mathbb{E}\mathcal{D}[\hat y_0]\right]^2 + \left(\mathbb{E}\mathcal{D}[\hat y_0]^2 - f(x_0)\right)^2 \ &amp;amp; = \text{Var}_\mathcal{D}(\hat y_0) + \text{Bias}(\hat y_0)^2 \end{align*} $$ where the bias is the displacement of the mean prediction $\hat y_0$ from the true target $y_0 = f(x_0)$, and the variance is the variance of the prediction.</description></item><item><title>Classical Machine Learning</title><link>https://balbonario.github.io/indices/classical-machine-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/indices/classical-machine-learning/</guid><description>Classical Machine Learning refers to the more ancient part of machine learning, which has many relations with statistics, and mainly deals with the problem of Function Approximation via Convex Optimization of some basic models.</description></item><item><title>Function Approximation</title><link>https://balbonario.github.io/machine-learning/function-approximation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/function-approximation/</guid><description>The setting of Function Approximation is the following: suppose you are given two spaces $X$ and $Y$, which model respectively the &amp;ldquo;independent variable&amp;rdquo; and the &amp;ldquo;dependent variable&amp;rdquo; or, more clearly, $X$ is the variable you can observe (such as concentrations of various molecules in blood) and $Y$ is the variable you are interested in predicting (such as the possibility of a stroke to that person).</description></item><item><title>Hastie, Tibshirani, Friedman - The Elements of Statistical Learning</title><link>https://balbonario.github.io/references/hastie-tibshirani-friedman-the-elements-of-statistical-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/references/hastie-tibshirani-friedman-the-elements-of-statistical-learning/</guid><description>A well-known introductory book in the field of Classical Machine Learning, which talks about basic topics such as Bias-Variance Decomposition, The Curse of Dimensionality, and standard models such as Linear Regression, K-Nearest Neighbours.</description></item><item><title>K-Nearest Neighbours</title><link>https://balbonario.github.io/machine-learning/k-nearest-neighbours/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/k-nearest-neighbours/</guid><description>$k$-nearest neighbours is a Function Approximation algorithm which assumes that the function to approximate is locally constant, and thus approximates the output variable $\hat y$ with a mean of the $k$ nearest neighbours, i.</description></item><item><title>Linear Regression</title><link>https://balbonario.github.io/machine-learning/linear-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/linear-regression/</guid><description>Linear regression is a Function Approximation algorithm which assumes that your function is linear, i.e. $\hat Y = \beta X$ where $X \in \mathbb{R}^{p \times n}, \hat Y \in \mathbb{R}^{m \times n}, \beta \in \mathbb{R}^{m \times p}$, and $\beta$ is the matrix of coefficients to be fit.</description></item><item><title>Loss Function</title><link>https://balbonario.github.io/machine-learning/loss-function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/loss-function/</guid><description>#todo How to choose a loss function?</description></item><item><title>Mean Squared Error</title><link>https://balbonario.github.io/machine-learning/mean-squared-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/mean-squared-error/</guid><description>The mean squared error is a Loss Function typically used in classical Function Approximation. Given $n$ datapoints $(x_i, y_i)$ and relative predictions made by a learning algorithm $\hat y_i = f(x_i)$, the loss function is defined as: $$\mathcal{L}(f) := \frac1n \sum_{i = 1}^n \left(y_i - f(x_i)\right)^2.</description></item><item><title>Regularization</title><link>https://balbonario.github.io/machine-learning/regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/regularization/</guid><description>Deepenings On Representer Theorems and Convex Regularization (Archived)</description></item><item><title>The Curse of Dimensionality</title><link>https://balbonario.github.io/machine-learning/the-curse-of-dimensionality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/the-curse-of-dimensionality/</guid><description>We talk about the curse of dimensionality when #todo</description></item></channel></rss>