<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Balbonario on</title><link>https://balbonario.github.io/</link><description>Recent content in Balbonario on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 31 Aug 2022 19:03:48 +0200</lastBuildDate><atom:link href="https://balbonario.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Algorithms and Data Structures</title><link>https://balbonario.github.io/indices/algorithms-and-data-structures/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/indices/algorithms-and-data-structures/</guid><description>#todo
Intermediate Tricks Sqrt Decomposition</description></item><item><title>Bagging</title><link>https://balbonario.github.io/machine-learning/bagging/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/bagging/</guid><description>Bagging is a set of model ensembling methods where multiple models of the same learner type are built in parallel over different random variations and their predictions aggregated together by averaging or majority vote.</description></item><item><title>Bias-Variance Decomposition</title><link>https://balbonario.github.io/machine-learning/bias-variance-decomposition/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/bias-variance-decomposition/</guid><description>The Bias-Variance Decomposition is a mathematical decomposition of the distributional error. For the Mean Squared Error loss we have that $$ \begin{align*} \text{MSE}(x_0) &amp;amp; = \mathbb{E}\mathcal{D}\left[f(x_0) - \hat y_0\right]^2 \ &amp;amp; = \mathbb{E}\mathcal{D}\left[\hat y_0 - \mathbb{E}\mathcal{D}[\hat y_0]\right]^2 + \left(\mathbb{E}\mathcal{D}[\hat y_0]^2 - f(x_0)\right)^2 \ &amp;amp; = \text{Var}_\mathcal{D}(\hat y_0) + \text{Bias}(\hat y_0)^2 \end{align*} $$ where the bias is the displacement of the mean prediction $\hat y_0$ from the true target $y_0 = f(x_0)$, and the variance is the variance of the prediction.</description></item><item><title>Bias-Variance Tradeoff</title><link>https://balbonario.github.io/machine-learning/bias-variance-tradeoff/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/bias-variance-tradeoff/</guid><description>The bias-variance tradeoff refers to a common phenomenon occurring in many statistical models where it is not possible to decrease both the bias and the variance of a model1 by changing the number of parameters of a model.</description></item><item><title>Boosting</title><link>https://balbonario.github.io/machine-learning/boosting/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/boosting/</guid><description>Boosting is a sequential ensembling method to aggregate different weak models. The idea is to fit models iteratively such that the training of successive models is focused on the most difficult observations to fit up to now, so that the bias of the composite learner is reduced, and thus it is a technique usually used on models with low variance but high bias, such as shallow decision trees.</description></item><item><title>Bootstrapping</title><link>https://balbonario.github.io/machine-learning/bootstrapping/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/bootstrapping/</guid><description>Bootstrapping is a technique to estimate the variability of a statistics from the empirical variability of that statistic between subsamples (extracted with replacement) of the empirical data, rather than from parametric assumptions.</description></item><item><title>Classical Machine Learning</title><link>https://balbonario.github.io/indices/classical-machine-learning/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/indices/classical-machine-learning/</guid><description>Classical Machine Learning refers to the more ancient part of machine learning, which has many relations with statistics, and mainly deals with the problem of Function Approximation via Convex Optimization of some basic models.</description></item><item><title>Confusion Matrix</title><link>https://balbonario.github.io/machine-learning/confusion-matrix/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/confusion-matrix/</guid><description>A confusion matrix is a good but simple metric that should always be used when dealing with classification problems. The matrix has on one axis the true label and on the other axis the predicted label.</description></item><item><title>Convex Optimization</title><link>https://balbonario.github.io/optimization-theory/convex-optimization/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/optimization-theory/convex-optimization/</guid><description>#todo</description></item><item><title>Cross-Validation</title><link>https://balbonario.github.io/machine-learning/cross-validation/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/cross-validation/</guid><description>Cross-Validation refers to the practice of performing multiples independent splits of the dataset into a training set and a test set, and repeat the training of a new model and its evaluation on each independent split, to later average the results.</description></item><item><title>Double Descent</title><link>https://balbonario.github.io/machine-learning/double-descent/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/double-descent/</guid><description>Deepenings Double Descent: A Mathematical Explanation (Archived)</description></item><item><title>Empirical Risk Minimization</title><link>https://balbonario.github.io/machine-learning/empirical-risk-minimization/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/empirical-risk-minimization/</guid><description>#todo</description></item><item><title>Fast Updates to Matrix Operations</title><link>https://balbonario.github.io/linear-algebra/fast-updates-to-matrix-operations/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/linear-algebra/fast-updates-to-matrix-operations/</guid><description>Given a matrix operation $h$ and a fixed matrix $A$ there are in certain cases better ways to compute $h(A + uv^T)$ from $h(A)$ than to recompute it.</description></item><item><title>Function Approximation</title><link>https://balbonario.github.io/machine-learning/function-approximation/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/function-approximation/</guid><description>The setting of Function Approximation is the following: suppose you are given two spaces $X$ and $Y$, which model respectively the &amp;ldquo;independent variable&amp;rdquo; and the &amp;ldquo;dependent variable&amp;rdquo; or, more clearly, $X$ is the variable you can observe (such as concentrations of various molecules in blood) and $Y$ is the variable you are interested in predicting (such as the possibility of a stroke to that person).</description></item><item><title>Loss Function</title><link>https://balbonario.github.io/machine-learning/loss-function/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/loss-function/</guid><description>#todo How to choose a loss function?</description></item><item><title>Overfitting and Underfitting</title><link>https://balbonario.github.io/machine-learning/overfitting-and-underfitting/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/overfitting-and-underfitting/</guid><description>Overfitting occurs when the model is so specific to the data on which it has been trained that it is no longer applicable to other random samplings from the dataset, and thus has an almost zero train error but with a high test error.</description></item><item><title>Regularization</title><link>https://balbonario.github.io/machine-learning/regularization/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/regularization/</guid><description>#todo Describe regularization
Deepenings On Representer Theorems and Convex Regularization (Archived)</description></item><item><title>ROC Curve</title><link>https://balbonario.github.io/machine-learning/roc-curve/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/roc-curve/</guid><description>The ROC (Receiver Operating Characteristic) Curve is defined with respect to a given class and encodes the curve of false positive, true positive when varying a threshold $T$ for a one-versus-rest classifier.</description></item><item><title>Sqrt Decomposition</title><link>https://balbonario.github.io/algorithms-and-data-structures/sqrt-decomposition/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/algorithms-and-data-structures/sqrt-decomposition/</guid><description>#todo Refers to sqrt decomposition in data structures.</description></item><item><title>Stacking</title><link>https://balbonario.github.io/machine-learning/stacking/</link><pubDate>Wed, 31 Aug 2022 19:03:48 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/stacking/</guid><description>Stacking is a model ensembling technique that is focused on aggregating heterogeneous weak learners with a meta-model that aggregates the decisions of weak models.</description></item><item><title>Hastie, Tibshirani, Friedman - The Elements of Statistical Learning</title><link>https://balbonario.github.io/references/hastie-tibshirani-friedman-the-elements-of-statistical-learning/</link><pubDate>Wed, 31 Aug 2022 13:08:56 +0200</pubDate><guid>https://balbonario.github.io/references/hastie-tibshirani-friedman-the-elements-of-statistical-learning/</guid><description>A well-known introductory book in the field of Classical Machine Learning, which talks about basic topics such as Bias-Variance Decomposition, The Curse of Dimensionality, and standard models such as Linear Regression, K-Nearest Neighbours.</description></item><item><title>K-Nearest Neighbours</title><link>https://balbonario.github.io/machine-learning/k-nearest-neighbours/</link><pubDate>Wed, 31 Aug 2022 13:08:56 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/k-nearest-neighbours/</guid><description>$k$-nearest neighbours is a Function Approximation algorithm which assumes that the function to approximate is locally constant, and thus approximates the output variable $\hat y$ with a mean of the $k$ nearest neighbours, i.</description></item><item><title>The Curse of Dimensionality</title><link>https://balbonario.github.io/machine-learning/the-curse-of-dimensionality/</link><pubDate>Wed, 31 Aug 2022 13:08:56 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/the-curse-of-dimensionality/</guid><description>We talk about the curse of dimensionality when #todo</description></item><item><title>Linear Regression</title><link>https://balbonario.github.io/machine-learning/linear-regression/</link><pubDate>Wed, 31 Aug 2022 10:19:45 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/linear-regression/</guid><description>Linear regression is a Function Approximation algorithm which assumes that your function is linear, i.e. $\hat Y = \beta X$ where $X \in \mathbb{R}^{p \times n}, \hat Y \in \mathbb{R}^{m \times n}, \beta \in \mathbb{R}^{m \times p}$, and $\beta$ is the matrix of coefficients to be fit.</description></item><item><title>Mean Squared Error</title><link>https://balbonario.github.io/machine-learning/mean-squared-error/</link><pubDate>Wed, 31 Aug 2022 10:19:45 +0200</pubDate><guid>https://balbonario.github.io/machine-learning/mean-squared-error/</guid><description>The mean squared error is a Loss Function typically used in classical Function Approximation. Given $n$ datapoints $(x_i, y_i)$ and relative predictions made by a learning algorithm $\hat y_i = f(x_i)$, the loss function is defined as: $$\mathcal{L}(f) := \frac1n \sum_{i = 1}^n \left(y_i - f(x_i)\right)^2.</description></item></channel></rss>