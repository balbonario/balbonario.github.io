<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine-learnings on</title><link>https://balbonario.github.io/machine-learning/</link><description>Recent content in Machine-learnings on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://balbonario.github.io/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Function Approximation</title><link>https://balbonario.github.io/machine-learning/function-approximation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/function-approximation/</guid><description>The setting of Function Approximation is the following: suppose you are given two spaces $X$ and $Y$, which model respectively the &amp;ldquo;independent variable&amp;rdquo; and the &amp;ldquo;dependent variable&amp;rdquo; or, more clearly, $X$ is the variable you can observe (such as concentrations of various molecules in blood) and $Y$ is the variable you are interested in predicting (such as the possibility of a stroke to that person).</description></item><item><title>K-Nearest Neighbours</title><link>https://balbonario.github.io/machine-learning/k-nearest-neighbours/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/k-nearest-neighbours/</guid><description>$k$-nearest neighbours is a Function Approximation algorithm which assumes that the function to approximate is locally constant, and thus approximates the output variable $\hat y$ with a mean of the $k$ nearest neighbours, i.</description></item><item><title>Linear Regression</title><link>https://balbonario.github.io/machine-learning/linear-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/linear-regression/</guid><description>Linear regression is a Function Approximation algorithm which assumes that your function is linear, i.e. $\hat Y = \beta X$ where $X \in \mathbb{R}^{p \times n}, \hat Y \in \mathbb{R}^{m \times n}, \beta \in \mathbb{R}^{m \times p}$, and $\beta$ is the matrix of coefficients to be fit.</description></item><item><title>Loss Function</title><link>https://balbonario.github.io/machine-learning/loss-function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/loss-function/</guid><description>#todo How to choose a loss function?</description></item><item><title>Mean Squared Error</title><link>https://balbonario.github.io/machine-learning/mean-squared-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://balbonario.github.io/machine-learning/mean-squared-error/</guid><description>The mean squared error is a Loss Function typically used in classical Function Approximation. Given $n$ datapoints $(x_i, y_i)$ and relative predictions made by a learning algorithm $\hat y_i = f(x_i)$, the loss function is defined as: $$\mathcal{L}(f) := \frac1n \sum_{i = 1}^n \left(y_i - f(x_i)\right)^2.</description></item></channel></rss>